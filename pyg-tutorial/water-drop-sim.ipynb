{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 2.1.1 with cuda 11.8\n",
      "PyTorch Geometric has version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# import torch_scatter\n",
    "# import torch_sparse\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch_geometric as pyg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import requests\n",
    "import stat\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import functools\n",
    "import networkx as nx\n",
    "\n",
    "import reading_utils\n",
    "\n",
    "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "print(f\"PyTorch Geometric has version {pyg.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataset\n",
    "class ParticleDataset(Dataset):\n",
    "    def __init__(self, dataset_name, split):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            dataset_name (_type_): _description_\n",
    "            split (_type_): _description_\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        # Import datasets from URL and store\n",
    "        base_url = f\"https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/{dataset_name}\"\n",
    "        self.file_types = [\"metadata.json\", \"train.tfrecord\", \"valid.tfrecord\", \"test.tfrecord\"]\n",
    "        self.raw_data_path = Path(f'C:/Users/Daniel/Documents/Python Projects/AiPEX-Projects/pyg-tutorial/data/raw') # Windows\n",
    "        \n",
    "        print('='*40)\n",
    "        print(f\"Downloading {dataset_name} dataset.\")\n",
    "    \n",
    "        for file_type in tqdm(self.file_types):\n",
    "            # output_file = Path(f'/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/data/raw/{dataset_name}/{file_type}')\n",
    "            output_file = self.raw_data_path / dataset_name / file_type\n",
    "      \n",
    "            if not os.path.exists(output_file.parent):\n",
    "                os.makedirs(output_file.parent) # Make parent directory for dataset if doesn't exist\n",
    "            \n",
    "            if not os.path.exists(output_file): # Write dataset to file only if it isn't downloaded\n",
    "                response = requests.get(f\"{base_url}/{file_type}\")\n",
    "                if(response.status_code==200):\n",
    "                    print(\"Downloading \", file_type, \"from \", dataset_name, \"...\")\n",
    "                    with open(output_file, mode='wb') as file:\n",
    "                        file.write(response.content)\n",
    "                else:\n",
    "                    print(f\"INVALID GET QUERY FOR URL: {base_url}/{file_type}\")\n",
    "            else:\n",
    "                print(output_file, \" already exists.\")\n",
    "        print('='*40)\n",
    "    \n",
    "    def _read_metadata(self):\n",
    "        with open(self.raw_data_path / self.dataset_name / \"metadata.json\", 'rt') as fp:\n",
    "            return json.loads(fp.read())\n",
    "    \n",
    "    def parse_data(self):\n",
    "        metadata = self._read_metadata()\n",
    "        match self.split:\n",
    "            case \"train\":\n",
    "                data_path = self.raw_data_path / self.dataset_name / 'train.tfrecord'\n",
    "            case \"valid\":\n",
    "                data_path = self.raw_data_path / self.dataset_name / 'valid.tfrecord'\n",
    "            case \"test\":\n",
    "                data_path = self.raw_data_path / self.dataset_name / 'test.tfrecord'\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid split: {self.split}\")\n",
    "            \n",
    "        ds = tf.data.TFRecordDataset([data_path])\n",
    "        ds = ds.map(functools.partial(reading_utils.parse_serialized_simulation_example, metadata=metadata))\n",
    "        lds = list(ds)\n",
    "        self.particle_types = lds[0][0]['particle_type'].numpy()\n",
    "        self.positions = lds[0][1]['position'].numpy()\n",
    "        \n",
    "        return self.particle_types, self.positions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.particle_types)\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Downloading WaterDrop dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  metadata.json from  WaterDrop ...\n",
      "Downloading  train.tfrecord from  WaterDrop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [05:16<01:42, 102.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  valid.tfrecord from  WaterDrop ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [05:26<00:00, 81.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  test.tfrecord from  WaterDrop ...\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295,)\n",
      "(1001, 295, 2)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "train_dataset=ParticleDataset(\"WaterDrop\", \"test\")\n",
    "particle_types, positions = train_dataset.parse_data()\n",
    "print(particle_types.shape)\n",
    "print(positions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(position_seq, noise_std):\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    time_steps = velocity_seq.size(1)\n",
    "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std/time_steps ** 0.5)\n",
    "    position_noise = velocity_noise.cumsum(dim=1) # Integrate velocity noise to get positional noise\n",
    "    position_noise.torch.cat(torch.zeros)\n",
    "    return position_noise\n",
    "\n",
    "def preprocess(particle_type, position_seq, target_position, metadata, noise_std):\n",
    "    position_noise = generate_noise(position_seq, noise_std)\n",
    "    position_seq = position_seq + position_noise\n",
    "    \n",
    "    # Compute velocity with positional noise\n",
    "    recent_position = position_seq[:,-1]\n",
    "    velocity_seq = position_seq[:,1:] - position_seq[:,:-1]\n",
    "    \n",
    "    # Construct the graph\n",
    "    num_particles = recent_position.size(0)\n",
    "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"],loop=True, max_num_neighbors=num_particles)\n",
    "    \n",
    "    # node-level features: velocity, distance to the boundary\n",
    "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
    "    boundary = torch.tensor(metadata[\"bounds\"])\n",
    "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
    "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
    "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
    "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
    "\n",
    "    # edge-level features: displacement, distance\n",
    "    dim = recent_position.size(-1)\n",
    "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
    "                torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
    "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
    "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
    "\n",
    "    # ground truth for training\n",
    "    if target_position is not None:\n",
    "        last_velocity = velocity_seq[:, -1]\n",
    "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
    "        acceleration = next_velocity - last_velocity\n",
    "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
    "    else:\n",
    "        acceleration = None\n",
    "\n",
    "    # return the graph with features\n",
    "    graph = pyg.data.Data(\n",
    "        x=particle_type,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
    "        y=acceleration,\n",
    "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
    "        )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7, noise_std=0.0, return_pos=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # load dataset from the disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        self.window_length = window_length\n",
    "        self.noise_std = noise_std\n",
    "        self.return_pos = return_pos\n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "\n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "        # cut particle trajectories according to time slices\n",
    "        self.windows = []\n",
    "        for traj in self.offset.values():\n",
    "            size = traj[\"position\"][\"shape\"][1]\n",
    "            length = traj[\"position\"][\"shape\"][0] - window_length + 1\n",
    "            for i in range(length):\n",
    "                desc = {\n",
    "                    \"size\": size,\n",
    "                    \"type\": traj[\"particle_type\"][\"offset\"],\n",
    "                    \"pos\": traj[\"position\"][\"offset\"] + i * size * self.dim,\n",
    "                }\n",
    "                self.windows.append(desc)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # load corresponding data for this time slice\n",
    "        window = self.windows[idx]\n",
    "        size = window[\"size\"]\n",
    "        particle_type = self.particle_type[window[\"type\"]: window[\"type\"] + size].copy()\n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        position_seq = self.position[window[\"pos\"]: window[\"pos\"] + self.window_length * size * self.dim].copy()\n",
    "        position_seq.resize(self.window_length, size, self.dim)\n",
    "        position_seq = position_seq.transpose(1, 0, 2)\n",
    "        target_position = position_seq[:, -1]\n",
    "        position_seq = position_seq[:, :-1]\n",
    "        target_position = torch.from_numpy(target_position)\n",
    "        position_seq = torch.from_numpy(position_seq)\n",
    "        \n",
    "        # construct the graph\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise_std)\n",
    "        if self.return_pos:\n",
    "          return graph, position_seq[:, -1]\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        # load data from the disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        self.window_length = window_length\n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "\n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.offset)\n",
    "\n",
    "    def get(self, idx):\n",
    "        traj = self.offset[idx]\n",
    "        size = traj[\"position\"][\"shape\"][1]\n",
    "        time_step = traj[\"position\"][\"shape\"][0]\n",
    "        particle_type = self.particle_type[traj[\"particle_type\"][\"offset\"]: traj[\"particle_type\"][\"offset\"] + size].copy()\n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        position = self.position[traj[\"position\"][\"offset\"]: traj[\"position\"][\"offset\"] + time_step * size * self.dim].copy()\n",
    "        position.resize(traj[\"position\"][\"shape\"])\n",
    "        position = torch.from_numpy(position)\n",
    "        data = {\"particle_type\": particle_type, \"position\": position}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/daniel/Documents/Python Projects/temp/metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetworkx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnx\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset_sample \u001b[39m=\u001b[39m OneStepDataset(base_output_dir, \u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m, return_pos\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m graph, position \u001b[39m=\u001b[39m dataset_sample[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe first item in the valid set is a graph: \u001b[39m\u001b[39m{\u001b[39;00mgraph\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# load dataset from the disk\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m_offset.json\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/GNN/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/daniel/Documents/Python Projects/temp/metadata.json'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dataset_sample = OneStepDataset(base_output_dir, \"valid\", return_pos=True)\n",
    "graph, position = dataset_sample[0]\n",
    "\n",
    "print(f\"The first item in the valid set is a graph: {graph}\")\n",
    "print(f\"This graph has {graph.num_nodes} nodes and {graph.num_edges} edges.\")\n",
    "print(f\"Each node is a particle and each edge is the interaction between two particles.\")\n",
    "print(f\"Each node has {graph.num_node_features} categorial feature (Data.x), which represents the type of the node.\")\n",
    "print(f\"Each node has a {graph.pos.size(1)}-dim feature vector (Data.pos), which represents the positions and velocities of the particle (node) in several frames.\")\n",
    "print(f\"Each edge has a {graph.num_edge_features}-dim feature vector (Data.edge_attr), which represents the relative distance and displacement between particles.\")\n",
    "print(f\"The model is expected to predict a {graph.y.size(1)}-dim vector for each node (Data.y), which represents the acceleration of the particle.\")\n",
    "\n",
    "# remove directions of edges, because it is a symmetric directed graph.\n",
    "nx_graph = pyg.utils.to_networkx(graph).to_undirected()\n",
    "# remove self loops, because every node has a self loop.\n",
    "nx_graph.remove_edges_from(nx.selfloop_edges(nx_graph))\n",
    "plt.figure(figsize=(7, 7))\n",
    "nx.draw(nx_graph, pos={i: tuple(v) for i, v in enumerate(position)}, node_size=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch_scatter\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"Multi-Layer perceptron\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            self.layers.append(torch.nn.Linear(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                output_size if i == layers - 1 else hidden_size,\n",
    "            ))\n",
    "            if i != layers - 1:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "        if layernorm:\n",
    "            self.layers.append(torch.nn.LayerNorm(output_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n",
    "                layer.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class InteractionNetwork(pyg.nn.MessagePassing):\n",
    "    def __init__(self, hidden_size, layers):\n",
    "        super().__init__()\n",
    "        self.lin_edge = MLP(hidden_size * 3, hidden_size, hidden_size, layers)\n",
    "        self.lin_node = MLP(hidden_size * 2, hidden_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_feature):\n",
    "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "        node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
    "        edge_out = edge_feature + edge_out\n",
    "        node_out = x + node_out\n",
    "        return node_out, edge_out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_feature):\n",
    "        x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
    "        x = self.lin_edge(x)\n",
    "        return x\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n",
    "        return (inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSimulator(torch.nn.Module):\n",
    "    \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=128,\n",
    "        n_mp_layers=10, # number of GNN layers\n",
    "        num_particle_types=9,\n",
    "        particle_type_dim=16, # embedding dimension of particle types\n",
    "        dim=2, # dimension of the world, typical 2D or 3D\n",
    "        window_size=5, # the model looks into W frames before the frame to be predicted\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embed_type = torch.nn.Embedding(num_particle_types, particle_type_dim)\n",
    "        self.node_in = MLP(particle_type_dim + dim * (window_size + 2), hidden_size, hidden_size, 3)\n",
    "        self.edge_in = MLP(dim + 1, hidden_size, hidden_size, 3)\n",
    "        self.node_out = MLP(hidden_size, hidden_size, dim, 3, layernorm=False)\n",
    "        self.n_mp_layers = n_mp_layers\n",
    "        self.layers = torch.nn.ModuleList([InteractionNetwork(\n",
    "            hidden_size, 3\n",
    "        ) for _ in range(n_mp_layers)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embed_type.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # pre-processing\n",
    "        # node feature: combine categorial feature data.x and contiguous feature data.pos.\n",
    "        node_feature = torch.cat((self.embed_type(data.x), data.pos), dim=-1)\n",
    "        node_feature = self.node_in(node_feature)\n",
    "        edge_feature = self.edge_in(data.edge_attr)\n",
    "        # stack of GNN layers\n",
    "        for i in range(self.n_mp_layers):\n",
    "            node_feature, edge_feature = self.layers[i](node_feature, data.edge_index, edge_feature=edge_feature)\n",
    "        # post-processing\n",
    "        out = self.node_out(node_feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = OUTPUT_DIR\n",
    "# model_path = os.path.join(\"temp\", \"models\", DATASET_NAME)\n",
    "# rollout_path = os.path.join(\"temp\", \"rollouts\", DATASET_NAME)\n",
    "\n",
    "# !mkdir -p \"$model_path\"\n",
    "# !mkdir -p \"$rollout_path\"\n",
    "\n",
    "# params = {\n",
    "#     \"epoch\": 1,\n",
    "#     \"batch_size\": 4,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"noise\": 3e-4,\n",
    "#     \"save_interval\": 1000,\n",
    "#     \"eval_interval\": 1000,\n",
    "#     \"rollout_interval\": 200000,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = Path(f'/Users/daniel/Documents/Python Projects/temp')\n",
    "if not os.path.exists(base_output_dir):\n",
    "    os.makedirs(base_output_dir)\n",
    "model_path = os.path.join(\"temp\", \"models\", dataset_name)\n",
    "rollout_path = os.path.join(\"temp\", \"rollouts\", dataset_name)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(rollout_path):\n",
    "    os.makedirs(rollout_path)\n",
    "\n",
    "params = {\n",
    "    \"epoch\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"noise\": 3e-4,\n",
    "    \"save_interval\": 1000,\n",
    "    \"eval_interval\": 1000,\n",
    "    \"rollout_interval\": 200000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, data, metadata, noise_std):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    window_size = model.window_size + 1\n",
    "    total_time = data[\"position\"].size(0)\n",
    "    traj = data[\"position\"][:window_size]\n",
    "    traj = traj.permute(1, 0, 2)\n",
    "    particle_type = data[\"particle_type\"]\n",
    "\n",
    "    for time in range(total_time - window_size):\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, traj[:, -window_size:], None, metadata, 0.0)\n",
    "            graph = graph.to(device)\n",
    "            acceleration = model(graph).cpu()\n",
    "            acceleration = acceleration * torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2) + torch.tensor(metadata[\"acc_mean\"])\n",
    "\n",
    "            recent_position = traj[:, -1]\n",
    "            recent_velocity = recent_position - traj[:, -2]\n",
    "            new_velocity = recent_velocity + acceleration\n",
    "            new_position = recent_position + new_velocity\n",
    "            traj = torch.cat((traj, new_position.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "def oneStepMSE(simulator, dataloader, metadata, noise):\n",
    "    \"\"\"Returns two values, loss and MSE\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        scale = torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise ** 2).cuda()\n",
    "        for data in valid_loader:\n",
    "            data = data.cuda()\n",
    "            pred = simulator(data)\n",
    "            mse = ((pred - data.y) * scale) ** 2\n",
    "            mse = mse.sum(dim=-1).mean()\n",
    "            loss = ((pred - data.y) ** 2).mean()\n",
    "            total_mse += mse.item()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    return total_loss / batch_count, total_mse / batch_count\n",
    "\n",
    "\n",
    "def rolloutMSE(simulator, dataset, noise):\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        for rollout_data in dataset:\n",
    "            rollout_out = rollout(simulator, rollout_data, dataset.metadata, noise)\n",
    "            rollout_out = rollout_out.permute(1, 0, 2)\n",
    "            loss = (rollout_out - rollout_data[\"position\"]) ** 2\n",
    "            loss = loss.sum(dim=-1).mean()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    return total_loss / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(params, simulator, train_loader, valid_loader, valid_rollout_dataset):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(simulator.parameters(), lr=params[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / 5e6))\n",
    "\n",
    "    # recording loss curve\n",
    "    train_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    onestep_mse_list = []\n",
    "    rollout_mse_list = []\n",
    "    total_step = 0\n",
    "\n",
    "    for i in range(params[\"epoch\"]):\n",
    "        simulator.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {i}\")\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for data in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            pred = simulator(data)\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count, \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "            total_step += 1\n",
    "            train_loss_list.append((total_step, loss.item()))\n",
    "\n",
    "            # evaluation\n",
    "            if total_step % params[\"eval_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                eval_loss, onestep_mse = oneStepMSE(simulator, valid_loader, valid_dataset.metadata, params[\"noise\"])\n",
    "                eval_loss_list.append((total_step, eval_loss))\n",
    "                onestep_mse_list.append((total_step, onestep_mse))\n",
    "                tqdm.write(f\"\\nEval: Loss: {eval_loss}, One Step MSE: {onestep_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # do rollout on valid set\n",
    "            if total_step % params[\"rollout_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                rollout_mse = rolloutMSE(simulator, valid_rollout_dataset, params[\"noise\"])\n",
    "                rollout_mse_list.append((total_step, rollout_mse))\n",
    "                tqdm.write(f\"\\nEval: Rollout MSE: {rollout_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # save model\n",
    "            if total_step % params[\"save_interval\"] == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": simulator.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(model_path, f\"checkpoint_{total_step}.pt\")\n",
    "                )\n",
    "    return train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/data/raw/WaterDrop/train_offset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_path \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/data/raw/\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m OneStepDataset(data_path, \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, noise_std\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mnoise\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m OneStepDataset(data_path, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, noise_std\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_loader \u001b[39m=\u001b[39m pyg\u001b[39m.\u001b[39mloader\u001b[39m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mmetadata.json\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00msplit\u001b[39m}\u001b[39;49;00m\u001b[39m_offset.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Python%20Projects/pytorchgeometric_tutorial/water-drop-sim.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m {\u001b[39mint\u001b[39m(k): v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/GNN/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/data/raw/WaterDrop/train_offset.json'"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data_path = Path(f'/Users/daniel/Documents/Python Projects/pytorchgeometric_tutorial/data/raw/{dataset_name}')\n",
    "train_dataset = OneStepDataset(data_path, \"train\", noise_std=params[\"noise\"])\n",
    "valid_dataset = OneStepDataset(data_path, \"valid\", noise_std=params[\"noise\"])\n",
    "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, pin_memory=True, num_workers=2)\n",
    "valid_loader = pyg.loader.DataLoader(valid_dataset, batch_size=params[\"batch_size\"], shuffle=False, pin_memory=True, num_workers=2)\n",
    "valid_rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "\n",
    "# build model\n",
    "simulator = LearnedSimulator()\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "# train the model\n",
    "train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list = train(params, simulator, train_loader, valid_loader, valid_rollout_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss curve\n",
    "plt.figure()\n",
    "plt.plot(*zip(*train_loss_list), label=\"train\")\n",
    "plt.plot(*zip(*eval_loss_list), label=\"valid\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
